{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller: Introducción a los Large Language Models (LLMs) usando Python\n",
    "\n",
    "## Objetivo\n",
    "El objetivo de este taller es introducir a los estudiantes a los conceptos básicos de los large language models (LLMs) y proporcionarles una experiencia práctica utilizando la biblioteca `transformers` de Hugging Face para trabajar con modelos preentrenados de lenguaje natural.\n",
    "\n",
    "## Contenido\n",
    "1. Introducción a los Large Language Models (LLMs)\n",
    "2. Configuración del entorno\n",
    "3. Carga y uso de un modelo preentrenado\n",
    "4. Ejemplos prácticos\n",
    "5. Ejercicio\n",
    "\n",
    "___\n",
    "\n",
    "## 1. Introducción a los Large Language Models (LLMs)\n",
    "\n",
    "Los large language models (LLMs) son modelos de inteligencia artificial que han sido entrenados en grandes cantidades de texto para comprender y generar lenguaje natural. Estos modelos pueden realizar una variedad de tareas de procesamiento de lenguaje natural (NLP) como traducción, resumen, generación de texto, respuesta a preguntas, entre otros.\n",
    "\n",
    "## 2. Configuración del entorno\n",
    "\n",
    "Para este taller, utilizaremos la biblioteca `transformers` de Hugging Face. Primero, necesitamos instalar las bibliotecas necesarias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'jupyter-server-p75pvpr9gqphrp7q'. Verify the server is running and reachable. (Connection got disposed.). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Instalación de la biblioteca transformers y torch\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga y uso de un modelo preentrenado\n",
    "\n",
    "Vamos a cargar un modelo preentrenado de Hugging Face y usarlo para generar texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far, far away, there seemed to me the existence of this invisible man, an obscure one who, from nothing of the world, only did his thoughts of the world have any definite connexion with any\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargamos un pipeline de generación de texto\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generamos texto con el modelo cargado\n",
    "text = generator(\"Once upon a time, in a land far, far away\", max_length=50, num_return_sequences=1)\n",
    "print(text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ejemplos prácticos\n",
    "\n",
    "### 4.1 Generación de texto\n",
    "Intenta generar texto con diferentes prompts y observa los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: In the future, AI will\n",
      "Generated Text: In the future, AI will be the future, as technology will soon become more complicated for humans to understand and interact with. The need for humans to learn and make intelligent decisions is increasing: we have been living with the same set of rules that we\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The secret to happiness is\n",
      "Generated Text: The secret to happiness is one that goes in at the base of all the pleasures. To create an emotional state is difficult enough, but to do so with your true mental state is a task far beyond that. The process of making peace with your sense\n",
      "\n",
      "Prompt: The quick brown fox\n",
      "Generated Text: The quick brown foxes have a great shot at catching the dogs that make their way along the coast, especially when it comes raining.\n",
      "\n",
      "The brown fox was sighted in September 2013 in Oregon, though it was never reported to the FBI until\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de generación de texto con diferentes prompts\n",
    "prompts = [\n",
    "    \"In the future, AI will\",\n",
    "    \"The secret to happiness is\",\n",
    "    \"The quick brown fox\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    text = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated Text: {text[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Respuesta a preguntas\n",
    "\n",
    "Podemos usar un pipeline de respuesta a preguntas para encontrar respuestas dentro de un contexto proporcionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¿Qué hacen los estudiantes en Yachay Tech?\n",
      "Respuesta: Ecuador\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargamos un pipeline de respuesta a preguntas\n",
    "qa_pipeline = pipeline('question-answering')\n",
    "\n",
    "# Definimos el contexto y la pregunta\n",
    "context = \"Yachay Tech es una universidad de investigación de Ecuador.\"\n",
    "question = \"¿Qué hacen los estudiantes en Yachay Tech?\"\n",
    "\n",
    "# Obtenemos la respuesta\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(f\"Pregunta: {question}\")\n",
    "print(f\"Respuesta: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ejercicio\n",
    "\n",
    "### 5.1 Generación de Resumen\n",
    "\n",
    "Usa un modelo preentrenado para generar un resumen de un texto largo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen:\n",
      " La inteligencia artificial (IA) se refiere a la simulación of the human intelligence . La característica ideal de la IAI is su capacidad for racionalizar y tomar acciones\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargamos un pipeline de resumen\n",
    "summarizer = pipeline('summarization')\n",
    "\n",
    "# Texto largo para resumir\n",
    "long_text = \"\"\"\n",
    "La inteligencia artificial (IA) se refiere a la simulación de la inteligencia humana en máquinas que están programadas para pensar como humanos y \n",
    "imitar sus acciones. El término también se puede aplicar a cualquier máquina que exhiba rasgos asociados con una mente humana como el aprendizaje y \n",
    "la resolución de problemas. La característica ideal de la inteligencia artificial es su capacidad para racionalizar y tomar acciones que tengan \n",
    "la mejor posibilidad de alcanzar un objetivo específico. Un subconjunto de la inteligencia artificial es el aprendizaje automático, que se refiere a \n",
    "la idea de que los sistemas informáticos pueden aprender de datos, identificar patrones y tomar decisiones con una mínima intervención humana.\n",
    "\"\"\"\n",
    "\n",
    "# Generamos el resumen\n",
    "summary = summarizer(long_text, max_length=50, min_length=25, do_sample=False)\n",
    "print(\"Resumen:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usa 3 de las otras tareas disponibles: \n",
    "\n",
    "- 'audio-classification'\n",
    "- 'automatic-speech-recognition'\n",
    "- 'conversational'\n",
    "- 'depth-estimation'\n",
    "- 'document-question-answering'\n",
    "- 'feature-extraction'\n",
    "- 'fill-mask'\n",
    "- 'image-classification'\n",
    "- 'image-feature-extraction'\n",
    "- 'image-segmentation'\n",
    "- 'image-to-image'\n",
    "- 'image-to-text'\n",
    "- 'mask-generation'\n",
    "- 'ner', 'object-detection'\n",
    "- 'question-answering'\n",
    "- 'sentiment-analysis'\n",
    "- 'summarization'\n",
    "- 'table-question-answering'\n",
    "- 'text-classification'\n",
    "- 'text-generation'\n",
    "- 'text-to-audio'\n",
    "- 'text-to-speech'\n",
    "- 'text2text-generation'\n",
    "- 'token-classification'\n",
    "- 'translation'\n",
    "- 'video-classification'\n",
    "- 'visual-question-answering'\n",
    "- 'vqa'\n",
    "- 'zero-shot-audio-classification'\n",
    "- 'zero-shot-classification'\n",
    "- 'zero-shot-image-classification'\n",
    "- 'zero-shot-object-detection'\n",
    "- 'translation_XX_to_YY'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
